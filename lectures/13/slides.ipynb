{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\nimport matplotlib.pyplot as plt\\nfrom IPython.display import set_matplotlib_formats\\n\\n%matplotlib inline\\nset_matplotlib_formats(\\\"png\\\")\\nplt.style.use(\\\"seaborn-white\\\")\\nimport numpy as np\\nimport seaborn as sns\\nfrom IPython.display import SVG\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\nimport matplotlib.pyplot as plt\\nfrom IPython.display import set_matplotlib_formats\\n\\n%matplotlib inline\\nset_matplotlib_formats(\\\"png\\\")\\nplt.style.use(\\\"seaborn-white\\\")\\nimport numpy as np\\nimport seaborn as sns\\nfrom IPython.display import SVG\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats(\"png\")\n",
    "plt.style.use(\"seaborn-white\")\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 12\n",
    "## Coalescent Hidden markov models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past few weeks, we studied inference methods which assume that evolution at each gene occurs indepenedently. We studied the allele frequency spectrum, which describes the distribution of the number of variant copies of a gene (Lecture 8), as well as the Pritchard-Stephens-Donnelly model (Lecture 10), which clusters together different samples based on a model that assumes complete independence between gene frequencies at different genes. \n",
    "\n",
    "This week, we'll go back to thinking about recombination and genes that evolve under the presence of linkage disequilibrium. We will use hidden Markov modeling to perform inference under this set of assumptions.\n",
    "\n",
    "We already saw an example of a hidden Markov model when we described the sequentially Markov coalescent (Lecture 6). In that lecture, we first generated a sequence of trees for two leaves, and then dropped mutations on them to generate data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extend this model in two ways:\n",
    "1. Still for sample size $n=2$, we will infer this sequence of trees, and use it to estimate the size of the population backwards in time.\n",
    "2. For sample size $n>2$, inferring trees becomes too complicated, so we will make a (strong) simplifying assumption regarding the nature of tree topologies, which leads to a simple yet powerful algorithm for inferring ancestry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bullet point #1 leads to the program PSMC ([Li & Durbin; *Nature*, 2011](https://www.nature.com/articles/nature10231)). Bullet point #2 leads to the method of Li & Stephens ([*Genetics*, 2003](https://www.genetics.org/content/165/4/2213.long)). Both of these papers are extremely influential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Li-Stephens haplotype copying model\n",
    "Li and Stephens (2003) were interested in computing the likelihood of a sample $H_1,\\dots,H_n$ of haplotypic data\n",
    "\n",
    "$$\\mathbb{P}_\\theta(H_1,\\dots,H_n),$$\n",
    "\n",
    "where $\\theta$ represents various parameters of some evolutionary model (in this case, recombination.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their insight was that, if we had a good approximation for the *conditional* probability $\\tilde{P}_\\theta(H_i\\mid H_{i-1},\\dots,H_1)$, then we could use it to perform likelihood-based inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule of probability, we have \n",
    "\\begin{align}\n",
    "\\mathbb{P}_\\theta(H_1,\\dots,H_n) &= \\pi_\\theta(H_1) \\prod_{i=2}^n \\mathbb{P}_\\theta(H_i\\mid H_{i-1},\\dots,H_1) \\\\\n",
    "&\\approx \\pi_\\theta(H_1) \\prod_{i=2}^n \\tilde{\\mathbb{P}}_\\theta(H_i\\mid H_{i-1},\\dots,H_1) \\\\\n",
    "&=: \\tilde{\\mathbb{P}}_\\theta(H_1,\\dots,H_n).\n",
    "\\end{align}\n",
    "\n",
    "This is known as the *product of approximation conditionals* (PAC) likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we approach $\\tilde{\\mathbb{P}}(H_i\\mid H_{i-1},\\dots,H_1)$?\n",
    "\n",
    "![li stephens](https://www.genetics.org/content/genetics/165/4/2213/F2.large.jpg)\n",
    "\n",
    "(graphic from original paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that each underlying haplotype is an imperfect mosaic of the preceding haplotypes. (Note: for simplicity, I will focus on the case where we have haplotypic data, however their model transfers readily to diploid genotype data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a haplotype $H_{n+1} = (H_{n+1,1},\\dots,H_{n+1,L})$, let $Y_\\ell\\in{1,\\dots,n}$ denote the identity of the copied-from haplotype at position $\\ell$. In the picture above, if we focus on $h_{4A}$, then $Y_1=Y_2=3$, $Y_3=Y_4=Y_5=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov modeling\n",
    "The LS method is naturally modeled using an hidden Markov model. The \"hidden state\" is $Y_i$ in the notation above, and the \"emission\" is the probability of observing $H_{n+1,\\ell}$ given that $Y_\\ell=h \\in \\{1,\\dots,n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood for an HMM is:\n",
    "\n",
    "$$p(H_{n+1,1}=a_1,\\dots,H_{n+1,L}=a_L)=\\sum_{h_1,\\dots,h_L} \\underbrace{p(Y_1=h_1)}_\\text{stationary}\\prod_{\\ell=2}^L \\underbrace{p(Y_\\ell=h_\\ell\\mid Y_{\\ell-1}=h_{\\ell-1})}_\\text{transition} \\prod_{i=1}^L p\\underbrace{(H_{n+1,\\ell}=a_\\ell \\mid Y_{\\ell}=h_\\ell)}_\\text{emission}.$$\n",
    "\n",
    "Note that the sum has exponentially terms, but we will use the Markov structure to evaluate it in only polynomial time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary distribution\n",
    "This is just $p(Y_1=h_1)$. We set this to $1/n$, i.e. uniform over $h=1,\\dots,n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission distribution\n",
    "Given that the target haplotype is currently copying off of haplotype $j\\in\\{1\\,\\dots,n\\}$, the emission probability is\n",
    "\n",
    "$$p(H_{n+1,\\ell}=H_{h_i,\\ell} \\mid {Y_i = h_i}) = 1-e^{-\\theta/n}.$$\n",
    "\n",
    "(Note this is a bit different than what they used in their original paper.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are currently copying from haplotype $h_\\ell$, what is the probability of $Y_{\\ell+1}$? With probability $e^{-\\rho/n}$, no recombination happens. Or, with probability $(1-e^{-\\rho/n})(1/n)$, a recombination happens but we recombine onto the same haplotype as before. Otherwise, we are equally probable to recombine onto all other haplotypes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(Y_{\\ell+1}=h_{\\ell+1}\\mid Y_\\ell=h_\\ell) = \\begin{cases}\n",
    "e^{-\\rho/n} + (1-e^{-\\rho/n})/n,&h_{\\ell+1}=h_\\ell\\\\\n",
    "(1-e^{-\\rho/n})/n,&\\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in matrix form, where $T_{ij} = p(Y_{\\ell+1}=j\\mid Y_\\ell=i)$, we have \n",
    "\n",
    "$$T=e^{-\\rho/n}I + \\frac{1-e^{-\\rho/n}}{n}\\mathbf{1}\\mathbf{1^T},$$\n",
    "\n",
    "i.e. diagonal plus rank-one. This will be useful soon, as it will lead to an order-of-magnitude speedup in the standard forward backward algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward-backward algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we saw that naively evaluating the likelihood of an HMM would require exponential time. However, by exploiting the Markov property and using dynamic programmming, we can reduce this to polynomial time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I adopt the notation of Bishop (section 13.2) here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the *forward* quantity \n",
    "\n",
    "$$\\hat{\\alpha}(z_n) = p(z_n\\mid x_1,\\dots,x_n),$$ \n",
    "\n",
    "i.e. the conditional probability of the hidden state $z_n$ given the first $n$ observations. It's easy to see that the following recursion holds:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\hat{\\alpha}(z_{n+1}) =\\frac{ \\sum_{z_n}  p(x_{n+1}\\mid{z_{n+1}})p(z_{n+1}\\mid z_n)\\hat{\\alpha}(z_n)}\n",
    "{p(x_{n+1} \\mid x_1,\\dots,x_n)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if we recursively compute\n",
    "\n",
    "$$\\hat{\\alpha}(z_{n+1}) \\propto p(x_{n+1}\\mid z_{n+1}) \\sum_{z_n}  p(z_{n+1}\\mid z_n)\\hat{\\alpha}(z_n)$$\n",
    "\n",
    "and store the normalizing constants $c_1\\,\\dots,c_N$, then \n",
    "\n",
    "$$\\mathbb{P}(x_1,\\dots,x_N) = \\prod_{i=1}^N c_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are interested in the posterior distribution \n",
    "\n",
    "$$p(z_n \\mid x_1,\\dots,x_N) = \\hat{\\alpha}(z_n) \\hat{\\beta}(z_n)$$\n",
    "\n",
    "where $$\\hat{\\beta}(z_n) = \\frac{p(x_{n+1},\\dots,x_N \\mid z_n)}{p(x_{n+1},\\dots,x_N\\mid x_1,\\dots,x_n)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\beta}(z_n)$ can be computed by a similar recursion:\n",
    "\n",
    "$$\\hat{\\beta}(z_n) = c^{-1}_{n+1}\\sum_{z_{n+1}} \\hat{\\beta}(z_{n+1}) p(x_{n+1}\\mid z_{n+1}) p(z_{n+1}\\mid z_n),$$\n",
    "\n",
    "where $c_{n+1}$ was computed during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the L-S model\n",
    "We will implemented the forward-backward algorithm and use it to obtain the posterior, which will give us a way to estimate ancestry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def ls_fb(D, pos, rho, theta):\n",
    "    \"Compute posterior distribution in Li-Stephens algorithm\"\n",
    "    L, n = D.shape\n",
    "\n",
    "    # emission probability\n",
    "    e = jnp.array([jnp.exp(-theta / n), -jnp.expm1(-theta / n)], dtype=jnp.float64)\n",
    "\n",
    "    alpha_hat_0 = (1.0 / n) * e[D[0]]\n",
    "    c0 = alpha_hat_0.sum()\n",
    "    alpha_hat_0 /= c0\n",
    "\n",
    "    # transition quantity\n",
    "    delta = jnp.diff(pos)\n",
    "    T = jnp.exp(-rho * delta / n)\n",
    "\n",
    "    def fwd(alpha_hat_i, x):\n",
    "        t, d = x\n",
    "        # alpha_hat[i+1] = e[d] * (alpha_hat[i] @ T)\n",
    "        #                = e[d] * (alpha_hat[i] @ (diag + const * 11^T))\n",
    "        #                = e[d] * (alpha_hat[i] * d + const), because |alpha_hat[i]| = 1\n",
    "        alpha_hat_i1 = e[d] * (t * alpha_hat_i + (1. - t) / n)\n",
    "        c = alpha_hat_i1.sum()\n",
    "        alpha_hat_i1 /= c\n",
    "        return alpha_hat_i1, (alpha_hat_i1, c)\n",
    "\n",
    "    _, (alpha_hat, c) = jax.lax.scan(fwd, alpha_hat_0, (T, D[1:]))\n",
    "    alpha_hat = jnp.concatenate([alpha_hat_0[None], alpha_hat])\n",
    "    c = jnp.concatenate([c0[None], c])\n",
    "\n",
    "    def bwd(beta_hat_i, x):\n",
    "        ci, t, d = x\n",
    "        beta_hat = e[d] * (t * beta_hat_i + (1. - t) / n * beta_hat_i.sum()) / ci\n",
    "        return beta_hat, beta_hat\n",
    "\n",
    "    beta_hat_L = jnp.ones_like(alpha_hat_0)\n",
    "    _, beta_hat = jax.lax.scan(bwd, beta_hat_L, (c[1:], T, D[1:]), reverse=True)\n",
    "    beta_hat = jnp.concatenate([beta_hat, beta_hat_L[None]])\n",
    "    return alpha_hat * beta_hat, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msprime as msp\n",
    "\n",
    "sim = msp.simulate(\n",
    "    sample_size=8, mutation_rate=1e-8, recombination_rate=1e-8, length=1e5, Ne=1e4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sim.genotype_matrix()\n",
    "pos = np.array([v.position for v in sim.variants()])\n",
    "D = abs(X[:3, 1:] - X[:3, :1])\n",
    "a, c = ls_fb(D, pos[:3], 1e-4, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.99536138, 1.00166592, 1.        ], dtype=float64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 1, 0, 1, 1]], dtype=int8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
